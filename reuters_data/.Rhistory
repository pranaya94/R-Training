description =jobs %>%
html_nodes("span.desc" ) %>%
html_text
url = jobs %>%
html_nodes(".desig")%>%
html_attrs()
df =data.frame(organization,jobtitle,experience,location)
View(df)
key= scan("~/Dropbox/March onwards/PDS V2/9 Text Mining/riya_twit_info.txt", what="character")
key
consumer_key= key[2]
consumer_secret= key[4]
access_token= key[6]
access_secret= key[8]
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
library("twitteR")
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
tweet=searchTwitter("#Tesla", n=10, lang="en")
tweets
tweet
df= twListToDF(tweets)
tweets=searchTwitter("#Tesla", n=10, lang="en")
df= twListToDF(tweets)
dim(df)
View(df)
tweets_bh= searchTwitter("#BanayeBehetarBharat", n=10, lang="en")
tweets_bh[1] # will show the first tweet
df2=twListToDF(tweets_bh)
View(df2)
install.packages("kknn")
library(tm)
library(SnowballC)
library(dplyr)
library(randomForest)
library(RXKCD)
library(wordcloud)
library(RColorBrewer)
library(Rcpp)
library(RCurl)
library(bitops)
library(twitteR)
library(ggplot2)
library(Rstem)
library(sentiment)
library(plyr)
require(stringr)
require(xts)
library(gdata)
library(xlsx)
library(class)
library(e1071)
library(openNLP)
library(openNLPmodels.en)
# messages=readLines("C:/Users/Riya/Dropbox/March onwards/Python Data Science/Data/SMSSpamCollection.txt")
messages=readLines("~/Dropbox/March onwards/Python Data Science/Data/SMSSpamCollection.txt")
print(messages)
messages =data.frame(messages,stringsAsFactors = F)
library(tidyr)
messages=messages %>% separate(messages,into=c("target","msg"),sep="\t")
glimpse(messages)
messages$msg[1]
messages$target[1]
messages$msg[3]
messages$target[3]
## converting all messages to a corpus
## #Corpus is a large collection of texts. It is a body of
# written or spoken material (txt,pdf,doc) upon which a linguistic analysis is based.
corpus= Corpus(VectorSource(messages$msg))
## data processing
trans=content_transformer(function (x , pattern ) gsub(pattern, " ", x))
#tm_map:apply transformation functions
corpus =tm_map(corpus, trans, "/")
corpus =tm_map(corpus, trans, "@")
corpus =tm_map(corpus, trans, "\\|")
## converting entire corpus to lowercase
corpus = tm_map(corpus, tolower)
##plain text does not contain any images/graphical representation
# and is in the readable form
corpus = tm_map(corpus, PlainTextDocument)
# writeLines(as.character(corpus),con="anyfilname.txt")
## Removing punctuation
corpus = tm_map(corpus, removePunctuation)
## Removing stopwords
corpus = tm_map(corpus, removeWords, stopwords("english"))
#A document-term matrix or term-document matrix is a mathematical
#matrix that describes the frequency of terms that occur in a
#collection of documents.
dtm = DocumentTermMatrix(corpus)
View(as.matrix(dtm))
spdtm = removeSparseTerms(dtm, 0.99)
spdtm
View(as.matrix(spdtm))
message_feats = as.data.frame(as.matrix(spdtm))
rm(dtm,spdtm)
message_feats$target = messages$target
message_feats$target=ifelse(message_feats$target=="spam",1,0)
messages$msg_len=nchar(messages$msg)
message_feats$msg_len=messages$msg_len
message_feats$target = as.factor(message_feats$target)
x=1:10
sample(x,3)
set.seed(2)
s=sample(1:nrow(message_feats),0.7*nrow(message_feats))
message_feats_train=message_feats[s,]
message_feats_test=message_feats[-s,]
View(messages)
View(message_feats_train)
?randomForest
library(randomForest)
rf_model=randomForest(target~.,data = message_feats_train,do.Trace=T)
rf_model=randomForest(target~.,data = message_feats_train,do.Trace=T)
colnames(message_feats) = make.names(colnames(message_feats))
set.seed(2)
s=sample(1:nrow(message_feats),0.7*nrow(message_feats))
message_feats_train=message_feats[s,]
message_feats_test=message_feats[-s,]
names(message_feats_train)
rf_model=randomForest(target~.,data = message_feats_train,do.Trace=T)
rf_test_predict = predict(rf_model,newdata=message_feats_test)
table(message_feats_test$target,rf_test_predict)
varImpPlot(rf_model)
msg_len_data=message_feats_train %>%
group_by(msg_len) %>%
summarise_each(funs(mean(.)))
msg_len_data$spam_prob=predict(rf_model,newdata=msg_len_data,type="prob")[,1]
library(ggplot2)
ggplot(msg_len_data,aes(x=msg_len,y=spam_prob))+geom_smooth()
predict(rf_model,newdata=msg_len_data,type="prob")
?findAssocs
word1=rep(1,5)
word2=c(0,1,1,1,1)
cor(word1,word2)
word1=c(0,0,1,1,1)
cor(word1,word2)
nb_model=naiveBayes(target~.,data = message_feats_train)
nb_test_predict = predict(nb_model,dplyr::select(message_feats_test,-target))
table(message_feats_test$target,nb_test_predict)
library(e1071)
svm_model= svm(target~.,data = message_feats_train,kernel='linear')
setwd("~/Dropbox/Trainings/DeloitteDec2016/reuters_data")
files_acq =list.files(getwd(),
pattern="training_acq.*\\.txt")
acq=lapply(files_acq,read.csv,header=F, stringsAsFactors = F)
file=data.frame("msg"=character(),"category"=character(),stringsAsFactors=FALSE)
for (i in 1:length(acq)){
d=acq[[i]]
d$collapse=apply(d,1,paste,collapse = " ")
k=paste(d$collapse,collapse = " ")
file[nrow(file)+1,]=c(k,"acq")
}
View(file)
table(message_feats_test$target,rf_test_predict)
nb_model=naiveBayes(target~.,data = message_feats_train)
nb_test_predict = predict(nb_model,dplyr::select(message_feats_test,-target))
nb_model
?naiveBayes
table(message_feats_test$target,nb_test_predict)
svm_model= svm(target~.,data = message_feats_train,kernel='linear')
svm_test_predict = predict(svm_model,message_feats_test)
table(message_feats_test$target,svm_test_predict)
svm_model_rbf= svm(target~.,data = message_feats_train)
svm_test_predict_rbf = predict(svm_model,message_feats_test)
table(message_feats_test$target,svm_test_predict_rbf)
?svm
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeWords, c("over", "term"))
corpus = tm_map(corpus, stemDocument)
dtm =DocumentTermMatrix(corpus,
control = list(weighting = function(x) weightTfIdf(x)))
spdtm = removeSparseTerms(dtm, 0.99)
message_feats = as.data.frame(as.matrix(spdtm))
View(message_feats)
colnames(message_feats) = make.names(colnames(message_feats))
message_feats$target = messages$target
message_feats$target=ifelse(message_feats$target=="spam",1,0)
message_feats$msg_len=nchar(messages$msg)
message_feats$target = as.factor(message_feats$target)
set.seed(2)
s=sample(1:nrow(message_feats),0.7*nrow(message_feats))
message_feats_train=message_feats[s,]
message_feats_test=message_feats[-s,]
rf_model=randomForest(target~.,data = message_feats_train)
rf_test_predict = predict(rf_model,message_feats_test)
table(message_feats_test$target,rf_test_predict)
nb_model=naiveBayes(target~.,data = message_feats_train)
nb_test_predict = predict(nb_model,dplyr::select(message_feats_test,-target))
table(message_feats_test$target,nb_test_predict)
svm_model= svm(target~.,data = message_feats_train)
svm_test_predict = predict(svm_model,message_feats_test)
table(message_feats_test$target,svm_test_predict)
setwd("~/Dropbox/Trainings/DeloitteDec2016/reuters_data")
setwd("~/Dropbox/Trainings/DeloitteDec2016/reuters_data")
files_acq =list.files(getwd(),
pattern="training_acq.*\\.txt")
acq=lapply(files_acq,read.csv,header=F, stringsAsFactors = F)
files_crude =list.files(getwd(),
pattern="training_crude.*\\.txt")
crude=lapply(files_crude,read.csv,header=F, stringsAsFactors = F)
files_earn =list.files(getwd(),
pattern="training_earn.*\\.txt")
earn=lapply(files_earn,read.csv,header=F, stringsAsFactors = F)
files_money =list.files(getwd(),
pattern="training_money.*\\.txt")
money=lapply(files_money,read.csv,header=F, stringsAsFactors = F)
files_grain =list.files(getwd(),
pattern="training_grain.*\\.txt")
grain=lapply(files_grain,read.csv,header=F, stringsAsFactors = F)
setwd("~/Dropbox/Trainings/DeloitteDec2016/reuters_data")
files_acq =list.files(getwd(),
pattern="training_acq.*\\.txt")
acq=lapply(files_acq,read.csv,header=F, stringsAsFactors = F)
# acq,money,grain,earn,crude
files_crude =list.files(getwd(),
pattern="training_crude.*\\.txt")
crude=lapply(files_crude,read.csv,header=F, stringsAsFactors = F)
files_earn =list.files(getwd(),
pattern="training_earn.*\\.txt")
earn=lapply(files_earn,read.csv,header=F, stringsAsFactors = F)
files_money =list.files(getwd(),
pattern="training_money.*\\.txt")
money=lapply(files_money,read.csv,header=F, stringsAsFactors = F)
files_grain =list.files(getwd(),
pattern="training_grain.*\\.txt")
grain=lapply(files_grain,read.csv,header=F, stringsAsFactors = F)
grain=lapply(files_grain,readLines)
grain[[1]]
files_acq =list.files(getwd(),
pattern="training_acq.*\\.txt")
acq=lapply(files_acq,readLines)
# acq,money,grain,earn,crude
files_crude =list.files(getwd(),
pattern="training_crude.*\\.txt")
crude=lapply(files_crude,readLines)
files_earn =list.files(getwd(),
pattern="training_earn.*\\.txt")
earn=lapply(files_earn,readLines)
files_money =list.files(getwd(),
pattern="training_money.*\\.txt")
money=lapply(files_money,readLines)
files_grain =list.files(getwd(),
pattern="training_grain.*\\.txt")
grain=lapply(files_grain,readLines)
file=data.frame("msg"=character(),"category"=character(),
stringsAsFactors=FALSE)
i=1
d=acq[[i]]
d
k=paste(d,collpse=" ")
k
length(k)
k=paste(d,collapse=" ")
k
for (i in 1:length(acq)){
d=acq[[i]]
k=paste(d,collapse=" ")
kfile[nrow(file)+1,]=c(k,"acq")
}
for (i in 1:length(acq)){
d=acq[[i]]
k=paste(d,collapse=" ")
file[nrow(file)+1,]=c(k,"acq")
}
View(file)
i=1
d=acq[[i]]
d
length(27)
length(d)
k=paste(d,collapse=" ")
k
file=data.frame("msg"=character(),"category"=character(),
stringsAsFactors=FALSE)
for (i in 1:length(acq)){
d=acq[[i]]
k=paste(d,collapse=" ")
file[nrow(file)+1,]=c(k,"acq")
}
for (i in 1:length(grain)){
d=grain[[i]]
k=paste(d,collapse=" ")
file[nrow(file)+1,]=c(k,"grain")
}
for (i in 1:length(crude)){
d=crude[[i]]
k=paste(d,collapse=" ")
file[nrow(file)+1,]=c(k,"crude")
}
for (i in 1:length(earn)){
d=earn[[i]]
k=paste(d,collapse = " ")
file[nrow(file)+1,]=c(k,"earn")
}
for (i in 1:length(money)){
d=money[[i]]
k=paste(d,collapse = " ")
file[nrow(file)+1,]=c(k,"money")
}
rm(acq,earn,money,grain,crude)
View(file)
x=1:10
x
paste(x,"b")
paste(x,collapse="$")
d
train_data=file
rm(file)
files_acq =list.files(getwd(),
pattern="test_acq.*\\.txt")
acq=lapply(files_acq,readLines)
files_crude =list.files(getwd(),
pattern="test_crude.*\\.txt")
crude=lapply(files_crude,readLines)
files_earn =list.files(getwd(),
pattern="test_earn.*\\.txt")
earn=lapply(files_earn,readLines)
files_money =list.files(getwd(),
pattern="test_money.*\\.txt")
money=lapply(files_money,readLines)
files_grain =list.files(getwd(),
pattern="test_grain.*\\.txt")
grain=lapply(files_grain,readLines)
file_test=data.frame("msg"=character(),"category"=character(),stringsAsFactors=FALSE)
for (i in 1:length(acq)){
d=acq[[i]]
k=paste(d,collapse = " ")
file_test[nrow(file_test)+1,]=c(k,"acq")
}
for (i in 1:length(grain)){
d=grain[[i]]
k=paste(d,collapse = " ")
file_test[nrow(file_test)+1,]=c(k,"grain")
}
for (i in 1:length(crude)){
d=crude[[i]]
k=paste(d,collapse = " ")
file_test[nrow(file_test)+1,]=c(k,"crude")
}
for (i in 1:length(earn)){
d=earn[[i]]
k=paste(d,collapse = " ")
file_test[nrow(file_test)+1,]=c(k,"earn")
}
for (i in 1:length(money)){
d=money[[i]]
k=paste(d,collapse = " ")
file_test[nrow(file_test)+1,]=c(k,"money")
}
test_data=file_test
rm(file_test)
corpus= Corpus(VectorSource(train_data$msg))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
spdtm = removeSparseTerms(dtm, 0.99)
rm(acq,money,grain,crude,earn)
rm(dtm,spdtm)
dtm = DocumentTermMatrix(corpus)
spdtm = removeSparseTerms(dtm, 0.99)
msgs = as.data.frame(as.matrix(spdtm))
rm(dtm,spdtm)
colnames(msgs) = make.names(colnames(msgs))
msgs$target = train_data$category
msgs$target = as.factor(msgs$target)
set.seed(2)
s=sample(1:nrow(msgs),0.7*nrow(msgs))
msgs_train_knn=msgs[s,]
msgs_test_knn=msgs[-s,]
cl_train=msgs_train_knn$target
cl_test=msgs_test_knn$target
msgs_train_knn=msgs_train_knn %>%
select(-target)
msgs_test_knn=msgs_test_knn %>%
select(-target)
library(class)
nrow(msgs_train_knn)
knn_model= knn(msgs_train_knn,msgs_test_knn,cl_train)
table("Predictions" = knn_model, Actual = cl_test)
table(Predictions = knn_model, Actual = cl_test)
getwd()
mydata=readLines("~/Dropbox/Trainings/DeloitteDec2016/amazon_reviews.txt")
mdata=str_trim(mydata)
mdata=str_trim(mydata)
mydata=readLines("~/Dropbox/Trainings/DeloitteDec2016/amazon_reviews.txt")
mdata=str_trim(mydata)
L=mdata!=""
mdata=mdata[L]
rm(mydata)
amz_data =data.frame(mdata,stringsAsFactors = F)
?str_trim
rm(mdata)
View(amz_data)
even_indices=seq(2,62000,by=10)
amaz_data = amz_data[even_indices,1]
rm(amz_data)
mydata=readLines("~/Dropbox/Trainings/DeloitteDec2016/amazon_reviews.txt")
mdata=str_trim(mydata)
L=mdata!=""
mdata=mdata[L]
rm(mydata)
amz_data =data.frame(mdata,stringsAsFactors = F)
rm(mdata)
even_indices=seq(2,62000,by=10)
amaz_data = data.frame(amz_data[even_indices,1])
rm(amz_data)
colnames(amaz_data)="reviews"
View(amaz_data)
myCorpus = Corpus(VectorSource(amaz_data$reviews))
### Removing junk values###
trans=content_transformer(function (x , pattern ) gsub(pattern, " ", x))
myCorpus =tm_map(myCorpus, trans, "&quot")
myCorpus =tm_map(myCorpus, trans, "<p>")
myCorpus =tm_map(myCorpus, trans, "<br>")
myCorpus=tm_map(myCorpus, tolower)
myCorpus=tm_map(myCorpus, removePunctuation)
myCorpus= tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus= tm_map(myCorpus, stripWhitespace)
myCorpus=tm_map(myCorpus, removeWords, letters)
myCorpus = tm_map(myCorpus, PlainTextDocument)
myCorpus = tm_map(myCorpus, stemDocument)
myCorpus = tm_map(myCorpus, removeNumbers)
dtm =TermDocumentMatrix(myCorpus)
m =as.matrix(dtm)
v = sort(rowSums(m),decreasing=TRUE)
d = data.frame(word = names(v),freq=v)
View(d)
wordcloud(words = d$word, freq = d$freq, min.freq = 500,
max.words=500, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 300,
max.words=500, random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
myCorpus[1]$content
myCorpus[[1]]$content
myCorpus[[2]]$content
myCorpus[[3]]$content
myCorpus[[5]]$content
myCorpus1= data.frame(text = sapply(myCorpus, '[[', "content"), stringsAsFactors = FALSE)
View(myCorpus1)
myCorpus
myCorpus[[3]]
myCorpus[[3]]$content
myCorpus1= data.frame(text = sapply(myCorpus, '[[', "content"), stringsAsFactors = FALSE)
myCorpus1 = myCorpus1[!is.na(myCorpus1)]
class_pol = classify_polarity(myCorpus1, algorithm="bayes")
head(class_pol)
polarity = class_pol[,4]
class_emo = classify_emotion(myCorpus1, algorithm="bayes", prior=1.0)
head(class_emo)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "neutral"
df = data.frame(reviews=myCorpus1, emotion=emotion, polarity=polarity,
stringsAsFactors=FALSE)
library(dplyr)
df2= df %>%
mutate(emotion=factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
glimpse(df2)
library(ggplot2)
ggplot(df2, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories",y= "number of Reviews")
ggplot(df2, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of Reviews")
emos = levels(factor(df2$emotion))
n_emos=length(emos)
emo.docs = rep("", n_emos)
for (i in 1:n_emos)
{
tmp = subset(myCorpus1,emotion == emos[i])
emo.docs[i] = paste(tmp, collapse=" ")
}
# create corpus
corpus = Corpus(VectorSource(emo.docs))
#remove white spaces
corpus= tm_map(corpus, stripWhitespace)
corpus= tm_map(corpus, tolower)
corpus= tm_map(corpus, removeWords, stopwords("english"))
corpus= tm_map(corpus, stemDocument,language = ("english"))
corpus= tm_map(corpus, removePunctuation)
corpus= tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, PlainTextDocument)
#create Term document matrix
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(n_emos, "Dark2"),
scale = c(3,.5), random.order = FALSE,
title.size = 1.5)
library(topicmodels)
lda_data=myCorpus[1:15]
dtm=TermDocumentMatrix(lda_data)
dtm_topic= as.DocumentTermMatrix(dtm)
lda= LDA(dtm_topic, k = 3)
lda
summary(lda)
term =terms(lda, 5)
term
topics= topics(lda,1)
topics
